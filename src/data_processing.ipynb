{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcafd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pprint import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a4a6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = json.load(open(\"../data/arc-agi_training_challenges.json\"))\n",
    "training_solutions = json.load(open(\"../data/arc-agi_training_solutions.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e175f78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [{'input': [[7, 9], [4, 3]],\n",
       "   'output': [[7, 9, 7, 9, 7, 9],\n",
       "    [4, 3, 4, 3, 4, 3],\n",
       "    [9, 7, 9, 7, 9, 7],\n",
       "    [3, 4, 3, 4, 3, 4],\n",
       "    [7, 9, 7, 9, 7, 9],\n",
       "    [4, 3, 4, 3, 4, 3]]},\n",
       "  {'input': [[8, 6], [6, 4]],\n",
       "   'output': [[8, 6, 8, 6, 8, 6],\n",
       "    [6, 4, 6, 4, 6, 4],\n",
       "    [6, 8, 6, 8, 6, 8],\n",
       "    [4, 6, 4, 6, 4, 6],\n",
       "    [8, 6, 8, 6, 8, 6],\n",
       "    [6, 4, 6, 4, 6, 4]]}],\n",
       " 'test': [{'input': [[3, 2], [7, 8]]}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['00576224']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df410dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[3, 2, 3, 2, 3, 2],\n",
       "  [7, 8, 7, 8, 7, 8],\n",
       "  [2, 3, 2, 3, 2, 3],\n",
       "  [8, 7, 8, 7, 8, 7],\n",
       "  [3, 2, 3, 2, 3, 2],\n",
       "  [7, 8, 7, 8, 7, 8]]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_solutions['00576224']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa08b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task():\n",
    "    def __init__(self, task, name):\n",
    "        self.tasks = [*task['train'], task['test']]\n",
    "        self.name = name\n",
    "\n",
    "class ARCDataset(Dataset):\n",
    "    def __init__(self, data: dict, test_set: bool = False):\n",
    "        if test_set:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "            for name, task in data.items():\n",
    "                self.data.append(Task(task, name))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7430d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.Conv2d(input_dim, hidden_dim, kernel_size=3, stride=1, padding=2))\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.gelu(layer(x))\n",
    "\n",
    "        x = torch.mean(x, dim=(2, 3))\n",
    "        return x\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.linear_layers.append(nn.Linear(output_dim, output_dim))\n",
    "\n",
    "    def get_2d_positional_encoding(self, height, width, d_model, device='cpu'):\n",
    "        \"\"\"\n",
    "        Generate 2D positional encoding for an image of size (height, width).\n",
    "        Returns a tensor of shape (height, width, d_model).\n",
    "        \"\"\"\n",
    "        if d_model % 4 != 0:\n",
    "            raise ValueError(\"d_model must be divisible by 4 for 2D positional encoding.\")\n",
    "\n",
    "        # (H, W, D)\n",
    "        pe = torch.zeros(height, width, d_model, device=device)\n",
    "        d_model_half = d_model // 2\n",
    "\n",
    "        # Positions\n",
    "        y_pos = torch.arange(height, device=device).unsqueeze(1).float()  # (H,1)\n",
    "        x_pos = torch.arange(width, device=device).unsqueeze(0).float()   # (1,W)\n",
    "\n",
    "        # frequency terms\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model_half, 2, device=device).float()\n",
    "            * (-(torch.log(torch.tensor(10000.0, device=device)) / d_model_half))\n",
    "        )  # (D/4,)\n",
    "\n",
    "        # Angles with explicit last-dim for broadcasting\n",
    "        y_angles = y_pos.unsqueeze(-1) * div_term  # (H,1,D/4)\n",
    "        x_angles = x_pos.unsqueeze(-1) * div_term  # (1,W,D/4)\n",
    "\n",
    "        # Sine/Cosine\n",
    "        y_sin, y_cos = torch.sin(y_angles), torch.cos(y_angles)  # (H,1,D/4)\n",
    "        x_sin, x_cos = torch.sin(x_angles), torch.cos(x_angles)  # (1,W,D/4)\n",
    "\n",
    "        # Assign: Y encodings in first half, X in second half\n",
    "        pe[:, :, 0:d_model_half:2] = y_sin\n",
    "        pe[:, :, 1:d_model_half:2] = y_cos\n",
    "        pe[:, :, d_model_half::2] = x_sin\n",
    "        pe[:, :, d_model_half + 1::2] = x_cos\n",
    "\n",
    "        return pe  # (height, width, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "302dc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_dataset = ARCDataset(training_data)\n",
    "training_loader = DataLoader(arc_dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b54b769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': [[7, 9], [4, 3]],\n",
      "  'output': [[7, 9, 7, 9, 7, 9],\n",
      "             [4, 3, 4, 3, 4, 3],\n",
      "             [9, 7, 9, 7, 9, 7],\n",
      "             [3, 4, 3, 4, 3, 4],\n",
      "             [7, 9, 7, 9, 7, 9],\n",
      "             [4, 3, 4, 3, 4, 3]]},\n",
      " {'input': [[8, 6], [6, 4]],\n",
      "  'output': [[8, 6, 8, 6, 8, 6],\n",
      "             [6, 4, 6, 4, 6, 4],\n",
      "             [6, 8, 6, 8, 6, 8],\n",
      "             [4, 6, 4, 6, 4, 6],\n",
      "             [8, 6, 8, 6, 8, 6],\n",
      "             [6, 4, 6, 4, 6, 4]]},\n",
      " [{'input': [[3, 2], [7, 8]]}]]\n"
     ]
    }
   ],
   "source": [
    "for x in arc_dataset:\n",
    "    pp(x.tasks)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6e40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36fd760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 7, 64])\n",
      "caught: d_model must be divisible by 4 for 2D positional e\n"
     ]
    }
   ],
   "source": [
    "dm = DiffusionModel(1, 32, 64, 2)\n",
    "pe = dm.get_2d_positional_encoding(5, 7, 64)\n",
    "print(pe.shape)\n",
    "try:\n",
    "    dm.get_2d_positional_encoding(5, 7, 62)\n",
    "except ValueError as e:\n",
    "    print(\"caught:\", str(e)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Utilities and Dataset for ARC A->B pairs ====\n",
    "import math\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "NUM_COLORS = 10  # ARC colors 0..9\n",
    "\n",
    "def grid_to_onehot(grid: List[List[int]]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert HxW int grid (values in [0,9]) to (C=10, H, W) one-hot float tensor.\n",
    "    \"\"\"\n",
    "    h = len(grid)\n",
    "    w = len(grid[0]) if h > 0 else 0\n",
    "    t = torch.tensor(grid, dtype=torch.long)\n",
    "    oh = F.one_hot(t, num_classes=NUM_COLORS).permute(2, 0, 1).float()  # (C,H,W)\n",
    "    return oh\n",
    "\n",
    "\n",
    "def onehot_to_labels(oh: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    (C,H,W) -> (H,W) argmax labels.\n",
    "    \"\"\"\n",
    "    return oh.argmax(dim=0)\n",
    "\n",
    "\n",
    "class ARCPairsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Produces (A, B, task_id) from ARC training data where A=input grid, B=output grid.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: Dict[str, Any]):\n",
    "        self.samples = []\n",
    "        for task_id, task in data.items():\n",
    "            for io in task.get('train', []):\n",
    "                A = io['input']\n",
    "                B = io['output']\n",
    "                self.samples.append({\n",
    "                    'task_id': task_id,\n",
    "                    'A': A,\n",
    "                    'B': B,\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        A_oh = grid_to_onehot(s['A'])  # (C,H,W)\n",
    "        B_oh = grid_to_onehot(s['B'])\n",
    "        A_lbl = torch.tensor(s['A'], dtype=torch.long)  # (H,W)\n",
    "        B_lbl = torch.tensor(s['B'], dtype=torch.long)\n",
    "        return {\n",
    "            'task_id': s['task_id'],\n",
    "            'A_oh': A_oh,\n",
    "            'B_oh': B_oh,\n",
    "            'A_lbl': A_lbl,\n",
    "            'B_lbl': B_lbl,\n",
    "            'A_size': A_lbl.shape,  # (H,W)\n",
    "            'B_size': B_lbl.shape,\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_varsize(batch: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Keep variable-sized tensors in lists; stack what's stackable.\n",
    "    \"\"\"\n",
    "    out = {\n",
    "        'task_id': [b['task_id'] for b in batch],\n",
    "        'A_oh': [b['A_oh'] for b in batch],\n",
    "        'B_oh': [b['B_oh'] for b in batch],\n",
    "        'A_lbl': [b['A_lbl'] for b in batch],\n",
    "        'B_lbl': [b['B_lbl'] for b in batch],\n",
    "        'A_size': [b['A_size'] for b in batch],\n",
    "        'B_size': [b['B_size'] for b in batch],\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Encoder-Decoder with shared weights ====\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=NUM_COLORS, d_model=128):\n",
    "        super().__init__()\n",
    "        # simple CNN to get global embedding\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.proj = nn.Linear(128, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B,C,H,W) or (C,H,W)\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        h = self.net(x)  # (B,128,H,W)\n",
    "        h = h.mean(dim=(2, 3))  # (B,128)\n",
    "        z = self.proj(h)  # (B,d_model)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z\n",
    "\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, out_channels=NUM_COLORS, d_model=128):\n",
    "        super().__init__()\n",
    "        # decode with learned constant spatial tokens + convs\n",
    "        self.d_model = d_model\n",
    "        self.pos_h = 30\n",
    "        self.pos_w = 30\n",
    "        self.pos = nn.Parameter(torch.randn(1, d_model, self.pos_h, self.pos_w) * 0.02)\n",
    "        self.z_proj = nn.Linear(d_model, 128)\n",
    "        self.pos_proj = nn.Conv2d(d_model, 128, kernel_size=1)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, out_channels, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z: torch.Tensor, out_size: Tuple[int, int]) -> torch.Tensor:\n",
    "        # z: (B,d_model), out: (B,C,H,W) logits resized to out_size\n",
    "        if z.dim() == 1:\n",
    "            z = z.unsqueeze(0)\n",
    "        B, D = z.shape\n",
    "        H, W = out_size\n",
    "        base = self.pos.expand(B, -1, -1, -1)  # (B,D,h,w)\n",
    "        base128 = F.relu(self.pos_proj(base))  # (B,128,h,w)\n",
    "        z_expand = self.z_proj(z).unsqueeze(-1).unsqueeze(-1)  # (B,128,1,1)\n",
    "        h = base128 + z_expand\n",
    "        logits = self.net(h)  # (B,C,h,w)\n",
    "        logits = F.interpolate(logits, size=(H, W), mode='nearest')\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DirectionARC(nn.Module):\n",
    "    def __init__(self, d_model=128):\n",
    "        super().__init__()\n",
    "        self.encoder = ConvEncoder(d_model=d_model)\n",
    "        self.decoder = ConvDecoder(d_model=d_model)\n",
    "\n",
    "    def encode(self, x_chw: torch.Tensor) -> torch.Tensor:\n",
    "        return self.encoder(x_chw)\n",
    "\n",
    "    def decode(self, z: torch.Tensor, out_size: Tuple[int, int]) -> torch.Tensor:\n",
    "        return self.decoder(z, out_size)  # logits (B,C,H,W)\n",
    "\n",
    "    def forward(self, A_oh: torch.Tensor, B_oh: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Tuple[int,int]]:\n",
    "        # Accept single example (C,H,W)\n",
    "        H, W = A_oh.shape[-2], A_oh.shape[-1]\n",
    "        zA = self.encode(A_oh)\n",
    "        zB = self.encode(B_oh)\n",
    "        logitsA = self.decode(zA, (H, W))\n",
    "        logitsB = self.decode(zB, (H, W))\n",
    "        return zA, zB, logitsB, (H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31febdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Losses: direction contrastive and reconstruction ====\n",
    "\n",
    "def cosine_sim(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8):\n",
    "    a = F.normalize(a, dim=-1)\n",
    "    b = F.normalize(b, dim=-1)\n",
    "    return (a * b).sum(dim=-1)\n",
    "\n",
    "\n",
    "def supervised_direction_loss(dirs: torch.Tensor, task_ids: List[str], temp: float = 0.1):\n",
    "    \"\"\"\n",
    "    Supervised contrastive loss on direction vectors. Positives are same-task pairs, negatives different tasks.\n",
    "    dirs: (N, d) normalized direction vectors\n",
    "    task_ids: list of length N\n",
    "    \"\"\"\n",
    "    N = dirs.size(0)\n",
    "    dirs = F.normalize(dirs, dim=-1)\n",
    "    # map task ids to ints\n",
    "    uniq = {tid: i for i, tid in enumerate(sorted(set(task_ids)))}\n",
    "    labels = torch.tensor([uniq[t] for t in task_ids], device=dirs.device)\n",
    "\n",
    "    sim = dirs @ dirs.t() / temp  # (N,N)\n",
    "    # mask self\n",
    "    eye = torch.eye(N, device=dirs.device, dtype=torch.bool)\n",
    "    sim = sim.masked_fill(eye, float('-inf'))\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        pos_mask = (labels == labels[i]) & (~eye[i])  # exclude self\n",
    "        num_pos = int(pos_mask.sum().item())\n",
    "        if num_pos == 0:\n",
    "            continue\n",
    "        # log-softmax over all j != i\n",
    "        logits_i = sim[i]  # (N,)\n",
    "        log_prob = logits_i - torch.logsumexp(logits_i, dim=0)\n",
    "        loss_i = -log_prob[pos_mask].mean()\n",
    "        loss_sum = loss_sum + loss_i\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        return torch.tensor(0.0, device=dirs.device, requires_grad=True)\n",
    "    return loss_sum / count\n",
    "\n",
    "\n",
    "def reconstruction_loss(logits: torch.Tensor, target_labels: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Cross-entropy over pixels. logits: (B,C,H,W) or (C,H,W). target: (H,W)\n",
    "    \"\"\"\n",
    "    if logits.dim() == 3:\n",
    "        logits = logits.unsqueeze(0)\n",
    "    B, C, H, W = logits.shape\n",
    "    target = target_labels.view(1, H, W).expand(B, -1, -1)\n",
    "    return F.cross_entropy(logits, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f92acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Minimal training loop (single step sanity check) ====\n",
    "from collections import defaultdict\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pairs_ds = ARCPairsDataset(training_data)\n",
    "# We'll create batches of size N and form two views per batch by shuffling indices.\n",
    "loader = DataLoader(pairs_ds, batch_size=8, shuffle=True, collate_fn=collate_varsize)\n",
    "\n",
    "model = DirectionARC(d_model=128).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "alpha_dir = 1.0\n",
    "alpha_rec = 1.0\n",
    "\n",
    "batch = next(iter(loader))\n",
    "# Prepare tensors on device; keep variable sizes per item\n",
    "zA_list, zB_list, zC_list, zD_list = [], [], [], []\n",
    "rec_logits_list, rec_targets_list = [], []\n",
    "\n",
    "# We create two groups in the same batch by splitting it in half.\n",
    "# Assumption: items from same task may repeat across epochs, but for this sanity we pair indices 0..k-1 with k..2k-1.\n",
    "Bsz = len(batch['A_oh'])\n",
    "mid = Bsz // 2 if Bsz >= 2 else Bsz\n",
    "idx1 = list(range(0, mid))\n",
    "idx2 = list(range(mid, min(Bsz, mid*2)))\n",
    "if len(idx2) < len(idx1):\n",
    "    idx1 = idx1[:len(idx2)]\n",
    "\n",
    "model.train()\n",
    "opt.zero_grad()\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    A_oh = batch['A_oh'][i].unsqueeze(0).to(device)  # (1,C,H,W)\n",
    "    B_oh = batch['B_oh'][i].unsqueeze(0).to(device)\n",
    "    C_oh = batch['A_oh'][j].unsqueeze(0).to(device)\n",
    "    D_oh = batch['B_oh'][j].unsqueeze(0).to(device)\n",
    "\n",
    "    zA = model.encode(A_oh)\n",
    "    zB = model.encode(B_oh)\n",
    "    zC = model.encode(C_oh)\n",
    "    zD = model.encode(D_oh)\n",
    "\n",
    "    zA_list.append(zA)\n",
    "    zB_list.append(zB)\n",
    "    zC_list.append(zC)\n",
    "    zD_list.append(zD)\n",
    "\n",
    "    # Reconstruction for B\n",
    "    H, W = batch['B_size'][i]\n",
    "    logitsB = model.decode(zB, (H, W))  # (1,C,H,W)\n",
    "    targetB = batch['B_lbl'][i].to(device)\n",
    "    rec_logits_list.append(logitsB)\n",
    "    rec_targets_list.append(targetB)\n",
    "\n",
    "if len(zA_list) == 0:\n",
    "    raise RuntimeError('Batch too small to form pairs for direction loss.')\n",
    "\n",
    "zA = torch.cat(zA_list, dim=0)\n",
    "zB = torch.cat(zB_list, dim=0)\n",
    "zC = torch.cat(zC_list, dim=0)\n",
    "zD = torch.cat(zD_list, dim=0)\n",
    "\n",
    "loss_dir = direction_contrastive_loss(zA, zB, zC, zD, temp=0.1)\n",
    "\n",
    "# Reconstruction loss over all collected targets\n",
    "loss_rec = 0.0\n",
    "for logits, tgt in zip(rec_logits_list, rec_targets_list):\n",
    "    loss_rec = loss_rec + reconstruction_loss(logits, tgt)\n",
    "loss_rec = loss_rec / max(1, len(rec_logits_list))\n",
    "\n",
    "loss = alpha_dir * loss_dir + alpha_rec * loss_rec\n",
    "loss.backward()\n",
    "opt.step()\n",
    "\n",
    "print({\n",
    "    'loss': float(loss.item()),\n",
    "    'loss_dir': float(loss_dir.item()),\n",
    "    'loss_rec': float(loss_rec.item()),\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc-prize",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
